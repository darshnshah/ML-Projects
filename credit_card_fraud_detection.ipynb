{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit Card Fraud Detection - Improved Approach\n",
    "\n",
    "This notebook implements credit card fraud detection using:\n",
    "- Feature selection techniques\n",
    "- Multiple approaches for handling class imbalance (SMOTE, class weights)\n",
    "- Two machine learning models (Logistic Regression & Random Forest)\n",
    "- Threshold optimization for better performance\n",
    "- Comprehensive evaluation metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Machine Learning Libraries\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Imbalanced Learning\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Evaluation Metrics\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, roc_auc_score, \n",
    "    roc_curve, precision_recall_curve, f1_score, \n",
    "    accuracy_score, precision_score, recall_score,\n",
    "    average_precision_score, matthews_corrcoef\n",
    ")\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('creditcard.csv')\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic information about the dataset\n",
    "print(\"Dataset Info:\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"\\nData types:\")\n",
    "print(df.dtypes.value_counts())\n",
    "print(f\"\\nMissing values:\")\n",
    "print(df.isnull().sum().sum())\n",
    "print(f\"\\nMemory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Class Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class distribution\n",
    "class_counts = df['Class'].value_counts()\n",
    "print(\"Class Distribution:\")\n",
    "print(f\"Normal transactions (Class 0): {class_counts[0]:,} ({class_counts[0]/len(df)*100:.3f}%)\")\n",
    "print(f\"Fraud transactions (Class 1): {class_counts[1]:,} ({class_counts[1]/len(df)*100:.3f}%)\")\n",
    "print(f\"Imbalance ratio: 1:{class_counts[0]//class_counts[1]}\")\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Bar plot\n",
    "class_counts.plot(kind='bar', ax=axes[0], color=['skyblue', 'lightcoral'])\n",
    "axes[0].set_title('Class Distribution (Count)')\n",
    "axes[0].set_xlabel('Class')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].tick_params(axis='x', rotation=0)\n",
    "\n",
    "# Pie chart\n",
    "axes[1].pie(class_counts.values, labels=['Normal (0)', 'Fraud (1)'], \n",
    "           autopct='%1.3f%%', colors=['skyblue', 'lightcoral'])\n",
    "axes[1].set_title('Class Distribution (Percentage)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Analysis and Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "X = df.drop('Class', axis=1)\n",
    "y = df['Class']\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print(f\"\\nFeatures: {list(X.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation with target variable\n",
    "correlations = X.corrwith(y).abs().sort_values(ascending=False)\n",
    "print(\"Top 15 features correlated with target:\")\n",
    "print(correlations.head(15))\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(12, 8))\n",
    "correlations.plot(kind='bar', color='steelblue')\n",
    "plt.title('Feature Correlation with Target Variable (Absolute Values)')\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Absolute Correlation')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Preprocessing and Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple feature engineering\n",
    "def create_additional_features(df):\n",
    "    \"\"\"Create additional features from existing ones\"\"\"\n",
    "    df_new = df.copy()\n",
    "    \n",
    "    # Time-based features\n",
    "    df_new['Hour'] = (df_new['Time'] / 3600) % 24\n",
    "    df_new['Day'] = (df_new['Time'] / 86400) % 7\n",
    "    \n",
    "    # Amount-based features\n",
    "    df_new['Amount_log'] = np.log1p(df_new['Amount'])\n",
    "    df_new['Amount_sqrt'] = np.sqrt(df_new['Amount'])\n",
    "    \n",
    "    # Statistical features from V1-V28\n",
    "    v_features = [f'V{i}' for i in range(1, 29)]\n",
    "    df_new['V_sum'] = df_new[v_features].sum(axis=1)\n",
    "    df_new['V_mean'] = df_new[v_features].mean(axis=1)\n",
    "    df_new['V_std'] = df_new[v_features].std(axis=1)\n",
    "    \n",
    "    return df_new\n",
    "\n",
    "# Apply feature engineering\n",
    "df_engineered = create_additional_features(df)\n",
    "print(f\"Original features: {df.shape[1]}\")\n",
    "print(f\"After feature engineering: {df_engineered.shape[1]}\")\n",
    "print(f\"New features added: {df_engineered.shape[1] - df.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update X with engineered features\n",
    "X_engineered = df_engineered.drop('Class', axis=1)\n",
    "y = df_engineered['Class']\n",
    "\n",
    "print(f\"Engineered features shape: {X_engineered.shape}\")\n",
    "print(f\"New features: {[col for col in X_engineered.columns if col not in X.columns]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data for feature selection\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X_engineered, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_temp.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature selection using SelectKBest\n",
    "k_best = 20  # Select top 20 features\n",
    "selector = SelectKBest(score_func=f_classif, k=k_best)\n",
    "X_selected = selector.fit_transform(X_temp, y_temp)\n",
    "selected_features = X_temp.columns[selector.get_support()].tolist()\n",
    "feature_scores = selector.scores_[selector.get_support()]\n",
    "\n",
    "print(f\"Selected {len(selected_features)} features:\")\n",
    "feature_df = pd.DataFrame({\n",
    "    'Feature': selected_features,\n",
    "    'Score': feature_scores\n",
    "}).sort_values('Score', ascending=False)\n",
    "\n",
    "print(feature_df)\n",
    "\n",
    "# Visualize feature scores\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.barh(range(len(feature_df)), feature_df['Score'])\n",
    "plt.yticks(range(len(feature_df)), feature_df['Feature'])\n",
    "plt.xlabel('F-Score')\n",
    "plt.title('Selected Features by F-Score')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Data Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data with selected features\n",
    "X_train_sel = X_temp[selected_features]\n",
    "X_test_sel = X_test[selected_features]\n",
    "\n",
    "print(f\"Training set with selected features: {X_train_sel.shape}\")\n",
    "print(f\"Test set with selected features: {X_test_sel.shape}\")\n",
    "\n",
    "# Scale the features\n",
    "scaler = RobustScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_sel)\n",
    "X_test_scaled = scaler.transform(X_test_sel)\n",
    "\n",
    "# Convert back to DataFrame for easier handling\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=selected_features)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=selected_features)\n",
    "\n",
    "print(\"\\nData scaling completed.\")\n",
    "print(f\"Training set mean: {X_train_scaled.mean().mean():.6f}\")\n",
    "print(f\"Training set std: {X_train_scaled.std().mean():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Class Imbalance Handling Approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Class distribution before balancing:\")\n",
    "print(y_temp.value_counts())\n",
    "print(f\"Imbalance ratio: 1:{y_temp.value_counts()[0] // y_temp.value_counts()[1]}\")\n",
    "\n",
    "# Approach 1: SMOTE (Synthetic Minority Oversampling)\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train_scaled, y_temp)\n",
    "\n",
    "print(\"\\nClass distribution after SMOTE:\")\n",
    "print(pd.Series(y_train_smote).value_counts())\n",
    "\n",
    "# Visualize the effect\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Before SMOTE\n",
    "y_temp.value_counts().plot(kind='bar', ax=axes[0], color=['skyblue', 'lightcoral'])\n",
    "axes[0].set_title('Class Distribution - Before SMOTE')\n",
    "axes[0].set_xlabel('Class')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].tick_params(axis='x', rotation=0)\n",
    "\n",
    "# After SMOTE\n",
    "pd.Series(y_train_smote).value_counts().plot(kind='bar', ax=axes[1], color=['skyblue', 'lightcoral'])\n",
    "axes[1].set_title('Class Distribution - After SMOTE')\n",
    "axes[1].set_xlabel('Class')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].tick_params(axis='x', rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models with different approaches to handle imbalance\n",
    "models_config = {\n",
    "    # Original models without class balancing\n",
    "    'Logistic Regression (Original)': {\n",
    "        'model': LogisticRegression(random_state=42, max_iter=1000),\n",
    "        'data_type': 'Original'\n",
    "    },\n",
    "    'Random Forest (Original)': {\n",
    "        'model': RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1),\n",
    "        'data_type': 'Original'\n",
    "    },\n",
    "    \n",
    "    # Models with class weights (alternative to SMOTE)\n",
    "    'Logistic Regression (Balanced)': {\n",
    "        'model': LogisticRegression(random_state=42, max_iter=1000, class_weight='balanced'),\n",
    "        'data_type': 'Original'\n",
    "    },\n",
    "    'Random Forest (Balanced)': {\n",
    "        'model': RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1, class_weight='balanced'),\n",
    "        'data_type': 'Original'\n",
    "    },\n",
    "    \n",
    "    # Models with SMOTE\n",
    "    'Logistic Regression (SMOTE)': {\n",
    "        'model': LogisticRegression(random_state=42, max_iter=1000),\n",
    "        'data_type': 'SMOTE'\n",
    "    },\n",
    "    'Random Forest (SMOTE)': {\n",
    "        'model': RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1),\n",
    "        'data_type': 'SMOTE'\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"Model configurations to evaluate: {len(models_config)}\")\n",
    "for name in models_config.keys():\n",
    "    print(f\"  • {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation function\n",
    "def evaluate_model(model, X_train, y_train, X_test, y_test, model_name=\"Model\"):\n",
    "    \"\"\"Comprehensive model evaluation\"\"\"\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    avg_precision = average_precision_score(y_test, y_pred_proba)\n",
    "    mcc = matthews_corrcoef(y_test, y_pred)\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    \n",
    "    # Specificity\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    \n",
    "    results = {\n",
    "        'Model': model_name,\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1-Score': f1,\n",
    "        'ROC-AUC': roc_auc,\n",
    "        'PR-AUC': avg_precision,\n",
    "        'MCC': mcc,\n",
    "        'Specificity': specificity,\n",
    "        'TP': tp,\n",
    "        'TN': tn,\n",
    "        'FP': fp,\n",
    "        'FN': fn,\n",
    "        'Model_Object': model,\n",
    "        'Predictions': y_pred,\n",
    "        'Probabilities': y_pred_proba\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"Evaluation function defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate all model configurations\n",
    "all_results = []\n",
    "\n",
    "print(\"Evaluating models...\\n\")\n",
    "\n",
    "for model_name, config in models_config.items():\n",
    "    print(f\"Training and evaluating: {model_name}\")\n",
    "    \n",
    "    model = config['model']\n",
    "    data_type = config['data_type']\n",
    "    \n",
    "    # Choose appropriate training data\n",
    "    if data_type == 'SMOTE':\n",
    "        X_train = X_train_smote\n",
    "        y_train = y_train_smote\n",
    "    else:\n",
    "        X_train = X_train_scaled\n",
    "        y_train = y_temp\n",
    "    \n",
    "    # Evaluate model\n",
    "    result = evaluate_model(\n",
    "        model, X_train, y_train,\n",
    "        X_test_scaled, y_test,\n",
    "        model_name\n",
    "    )\n",
    "    result['Data_Type'] = data_type\n",
    "    result['Base_Model'] = model_name.split(' (')[0]\n",
    "    all_results.append(result)\n",
    "    \n",
    "    print(f\"  F1-Score: {result['F1-Score']:.4f}, ROC-AUC: {result['ROC-AUC']:.4f}\")\n",
    "    print(f\"  Precision: {result['Precision']:.4f}, Recall: {result['Recall']:.4f}\\n\")\n",
    "\n",
    "print(f\"Total evaluations completed: {len(all_results)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Results Analysis and Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results DataFrame\n",
    "results_df = pd.DataFrame(all_results)\n",
    "metrics_cols = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC', 'PR-AUC', 'MCC', 'Specificity']\n",
    "display_df = results_df[['Model', 'Base_Model', 'Data_Type'] + metrics_cols].copy()\n",
    "\n",
    "# Round numerical columns\n",
    "for col in metrics_cols:\n",
    "    display_df[col] = display_df[col].round(4)\n",
    "\n",
    "# Sort by F1-Score\n",
    "display_df_sorted = display_df.sort_values('F1-Score', ascending=False)\n",
    "\n",
    "print(\"Model Performance Comparison (Sorted by F1-Score):\")\n",
    "print(display_df_sorted)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"KEY INSIGHTS:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Compare Logistic Regression approaches\n",
    "lr_results = display_df[display_df['Base_Model'] == 'Logistic Regression'].sort_values('F1-Score', ascending=False)\n",
    "print(\"\\nLogistic Regression Performance:\")\n",
    "for idx, row in lr_results.iterrows():\n",
    "    approach = row['Model'].split('(')[1].replace(')', '')\n",
    "    print(f\"  {approach:10}: F1={row['F1-Score']:.4f}, Precision={row['Precision']:.4f}, Recall={row['Recall']:.4f}\")\n",
    "\n",
    "# Compare Random Forest approaches\n",
    "rf_results = display_df[display_df['Base_Model'] == 'Random Forest'].sort_values('F1-Score', ascending=False)\n",
    "print(\"\\nRandom Forest Performance:\")\n",
    "for idx, row in rf_results.iterrows():\n",
    "    approach = row['Model'].split('(')[1].replace(')', '')\n",
    "    print(f\"  {approach:10}: F1={row['F1-Score']:.4f}, Precision={row['Precision']:.4f}, Recall={row['Recall']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance comparison visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(18, 14))\n",
    "\n",
    "metrics_to_plot = ['F1-Score', 'ROC-AUC', 'Precision', 'Recall']\n",
    "\n",
    "for i, metric in enumerate(metrics_to_plot):\n",
    "    ax = axes[i//2, i%2]\n",
    "    \n",
    "    # Create grouped bar plot\n",
    "    model_names = [result['Model'] for result in all_results]\n",
    "    metric_values = [result[metric] for result in all_results]\n",
    "    \n",
    "    # Color code by approach\n",
    "    colors = []\n",
    "    for name in model_names:\n",
    "        if 'Original' in name:\n",
    "            colors.append('lightblue')\n",
    "        elif 'Balanced' in name:\n",
    "            colors.append('orange')\n",
    "        elif 'SMOTE' in name:\n",
    "            colors.append('lightcoral')\n",
    "    \n",
    "    bars = ax.bar(range(len(model_names)), metric_values, color=colors)\n",
    "    ax.set_xticks(range(len(model_names)))\n",
    "    ax.set_xticklabels([name.replace(' (', '\\n(') for name in model_names], \n",
    "                       rotation=45, ha='right', fontsize=9)\n",
    "    ax.set_ylabel(metric)\n",
    "    ax.set_title(f'{metric} Comparison Across Approaches')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, value in zip(bars, metric_values):\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(metric_values)*0.01,\n",
    "               f'{value:.3f}', ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "# Add legend\n",
    "legend_elements = [\n",
    "    plt.Rectangle((0,0),1,1, facecolor='lightblue', label='Original Data'),\n",
    "    plt.Rectangle((0,0),1,1, facecolor='orange', label='Class Weights'),\n",
    "    plt.Rectangle((0,0),1,1, facecolor='lightcoral', label='SMOTE')\n",
    "]\n",
    "fig.legend(handles=legend_elements, loc='center', bbox_to_anchor=(0.5, 0.02), ncol=3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(bottom=0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best model analysis\n",
    "best_result = max(all_results, key=lambda x: x['F1-Score'])\n",
    "\n",
    "print(f\"🏆 BEST PERFORMING MODEL: {best_result['Model']}\")\n",
    "print(\"\\nPerformance Metrics:\")\n",
    "for metric in metrics_cols:\n",
    "    value = best_result[metric]\n",
    "    print(f\"  {metric}: {value:.4f}\")\n",
    "\n",
    "# Confusion Matrix for best model\n",
    "cm = confusion_matrix(y_test, best_result['Predictions'])\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "           xticklabels=['Normal', 'Fraud'], \n",
    "           yticklabels=['Normal', 'Fraud'])\n",
    "plt.title(f'Confusion Matrix - {best_result[\"Model\"]}')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nConfusion Matrix Analysis:\")\n",
    "print(f\"  True Negatives (Correctly identified normal): {best_result['TN']:,}\")\n",
    "print(f\"  False Positives (False alarms): {best_result['FP']:,}\")\n",
    "print(f\"  False Negatives (Missed fraud): {best_result['FN']:,}\")\n",
    "print(f\"  True Positives (Correctly identified fraud): {best_result['TP']:,}\")\n",
    "\n",
    "print(f\"\\nModel Performance:\")\n",
    "print(f\"  Detection Rate: {best_result['TP']/(best_result['TP']+best_result['FN'])*100:.1f}%\")\n",
    "print(f\"  False Alarm Rate: {best_result['FP']/(best_result['FP']+best_result['TN'])*100:.3f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC and Precision-Recall curves comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Define colors and styles for different approaches\n",
    "approach_styles = {\n",
    "    'Original': {'color': 'blue', 'linestyle': '-'},\n",
    "    'Balanced': {'color': 'orange', 'linestyle': '--'},\n",
    "    'SMOTE': {'color': 'red', 'linestyle': ':'}\n",
    "}\n",
    "\n",
    "# ROC Curves\n",
    "for result in all_results:\n",
    "    fpr, tpr, _ = roc_curve(y_test, result['Probabilities'])\n",
    "    auc_score = result['ROC-AUC']\n",
    "    \n",
    "    # Determine style based on data type\n",
    "    if 'Balanced' in result['Model']:\n",
    "        style = approach_styles['Balanced']\n",
    "    elif 'SMOTE' in result['Model']:\n",
    "        style = approach_styles['SMOTE']\n",
    "    else:\n",
    "        style = approach_styles['Original']\n",
    "    \n",
    "    axes[0].plot(fpr, tpr, label=f\"{result['Model']} (AUC: {auc_score:.3f})\",\n",
    "                color=style['color'], linestyle=style['linestyle'])\n",
    "\n",
    "axes[0].plot([0, 1], [0, 1], 'k--', alpha=0.6, label='Random Classifier')\n",
    "axes[0].set_xlabel('False Positive Rate')\n",
    "axes[0].set_ylabel('True Positive Rate')\n",
    "axes[0].set_title('ROC Curves - All Models')\n",
    "axes[0].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Precision-Recall Curves\n",
    "for result in all_results:\n",
    "    precision, recall, _ = precision_recall_curve(y_test, result['Probabilities'])\n",
    "    pr_auc = result['PR-AUC']\n",
    "    \n",
    "    # Determine style based on data type\n",
    "    if 'Balanced' in result['Model']:\n",
    "        style = approach_styles['Balanced']\n",
    "    elif 'SMOTE' in result['Model']:\n",
    "        style = approach_styles['SMOTE']\n",
    "    else:\n",
    "        style = approach_styles['Original']\n",
    "    \n",
    "    axes[1].plot(recall, precision, label=f\"{result['Model']} (AUC: {pr_auc:.3f})\",\n",
    "                color=style['color'], linestyle=style['linestyle'])\n",
    "\n",
    "# Baseline for PR curve\n",
    "baseline = (y_test == 1).sum() / len(y_test)\n",
    "axes[1].axhline(y=baseline, color='k', linestyle='--', alpha=0.6, \n",
    "               label=f'Random Classifier ({baseline:.3f})')\n",
    "axes[1].set_xlabel('Recall')\n",
    "axes[1].set_ylabel('Precision')\n",
    "axes[1].set_title('Precision-Recall Curves - All Models')\n",
    "axes[1].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance for the best Random Forest model\n",
    "rf_results = [r for r in all_results if 'Random Forest' in r['Model']]\n",
    "\n",
    "if rf_results:\n",
    "    # Get the best Random Forest model\n",
    "    best_rf = max(rf_results, key=lambda x: x['F1-Score'])\n",
    "    rf_model = best_rf['Model_Object']\n",
    "    \n",
    "    # Get feature importance\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'Feature': selected_features,\n",
    "        'Importance': rf_model.feature_importances_\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    # Plot feature importance\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    top_features = feature_importance.head(15)\n",
    "    plt.barh(range(len(top_features)), top_features['Importance'])\n",
    "    plt.yticks(range(len(top_features)), top_features['Feature'])\n",
    "    plt.xlabel('Feature Importance')\n",
    "    plt.title(f'Top 15 Feature Importance - {best_rf[\"Model\"]}')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Top 10 Most Important Features ({best_rf['Model']}):\")\n",
    "    print(feature_importance.head(10))\n",
    "else:\n",
    "    print(\"No Random Forest models found in results.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Threshold Optimization (Optional Enhancement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Threshold optimization for the best model\n",
    "print(\"Optimizing decision threshold for the best model...\")\n",
    "\n",
    "best_model = best_result['Model_Object']\n",
    "best_probabilities = best_result['Probabilities']\n",
    "\n",
    "# Test different thresholds\n",
    "thresholds = np.arange(0.1, 0.9, 0.05)\n",
    "threshold_results = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    y_pred_thresh = (best_probabilities >= threshold).astype(int)\n",
    "    \n",
    "    # Calculate metrics for this threshold\n",
    "    precision = precision_score(y_test, y_pred_thresh)\n",
    "    recall = recall_score(y_test, y_pred_thresh)\n",
    "    f1 = f1_score(y_test, y_pred_thresh)\n",
    "    \n",
    "    threshold_results.append({\n",
    "        'Threshold': threshold,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1-Score': f1\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame\n",
    "threshold_df = pd.DataFrame(threshold_results)\n",
    "best_threshold = threshold_df.loc[threshold_df['F1-Score'].idxmax(), 'Threshold']\n",
    "best_f1_optimized = threshold_df['F1-Score'].max()\n",
    "\n",
    "print(f\"Best threshold: {best_threshold:.3f}\")\n",
    "print(f\"Optimized F1-Score: {best_f1_optimized:.4f}\")\n",
    "print(f\"Original F1-Score: {best_result['F1-Score']:.4f}\")\n",
    "print(f\"Improvement: {best_f1_optimized - best_result['F1-Score']:+.4f}\")\n",
    "\n",
    "# Plot threshold optimization\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(threshold_df['Threshold'], threshold_df['Precision'], 'b-', label='Precision', linewidth=2)\n",
    "plt.plot(threshold_df['Threshold'], threshold_df['Recall'], 'r-', label='Recall', linewidth=2)\n",
    "plt.plot(threshold_df['Threshold'], threshold_df['F1-Score'], 'g-', label='F1-Score', linewidth=2)\n",
    "plt.axvline(x=best_threshold, color='black', linestyle='--', alpha=0.7, label=f'Best Threshold ({best_threshold:.3f})')\n",
    "plt.axvline(x=0.5, color='gray', linestyle=':', alpha=0.5, label='Default Threshold (0.5)')\n",
    "plt.xlabel('Decision Threshold')\n",
    "plt.ylabel('Score')\n",
    "plt.title(f'Threshold Optimization - {best_result[\"Model\"]}')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Summary and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive summary report\n",
    "print(\"CREDIT CARD FRAUD DETECTION - IMPROVED ANALYSIS SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\n📊 DATASET OVERVIEW:\")\n",
    "print(f\"  • Total transactions: {len(df):,}\")\n",
    "print(f\"  • Normal transactions: {(df['Class'] == 0).sum():,} ({(df['Class'] == 0).mean()*100:.3f}%)\")\n",
    "print(f\"  • Fraud transactions: {(df['Class'] == 1).sum():,} ({(df['Class'] == 1).mean()*100:.3f}%)\")\n",
    "print(f\"  • Original features: {len(X.columns)}\")\n",
    "print(f\"  • Engineered features: {len(X_engineered.columns)}\")\n",
    "print(f\"  • Selected features: {len(selected_features)}\")\n",
    "\n",
    "print(f\"\\n🔍 FEATURE SELECTION:\")\n",
    "print(f\"  • Method used: SelectKBest with f_classif\")\n",
    "print(f\"  • Top 5 features: {feature_df['Feature'].head(5).tolist()}\")\n",
    "\n",
    "print(f\"\\n⚖️ IMBALANCE HANDLING APPROACHES TESTED:\")\n",
    "print(f\"  • Original data (no balancing)\")\n",
    "print(f\"  • Class weights (balanced)\")\n",
    "print(f\"  • SMOTE (synthetic oversampling)\")\n",
    "\n",
    "print(f\"\\n🤖 MODELS EVALUATED:\")\n",
    "print(f\"  • Logistic Regression (3 approaches)\")\n",
    "print(f\"  • Random Forest (3 approaches)\")\n",
    "print(f\"  • Total combinations: {len(all_results)}\")\n",
    "\n",
    "print(f\"\\n🏆 BEST MODEL PERFORMANCE:\")\n",
    "print(f\"  • Best Model: {best_result['Model']}\")\n",
    "print(f\"  • F1-Score: {best_result['F1-Score']:.4f}\")\n",
    "print(f\"  • ROC-AUC: {best_result['ROC-AUC']:.4f}\")\n",
    "print(f\"  • Precision: {best_result['Precision']:.4f}\")\n",
    "print(f\"  • Recall: {best_result['Recall']:.4f}\")\n",
    "\n",
    "if 'best_f1_optimized' in locals():\n",
    "    print(f\"  • Optimized F1-Score: {best_f1_optimized:.4f} (with threshold {best_threshold:.3f})\")\n",
    "\n",
    "print(f\"\\n📈 KEY FINDINGS:\")\n",
    "\n",
    "# Compare Logistic Regression approaches\n",
    "lr_original = [r for r in all_results if r['Model'] == 'Logistic Regression (Original)'][0]\n",
    "lr_balanced = [r for r in all_results if r['Model'] == 'Logistic Regression (Balanced)'][0]\n",
    "lr_smote = [r for r in all_results if r['Model'] == 'Logistic Regression (SMOTE)'][0]\n",
    "\n",
    "print(f\"\\nLogistic Regression F1-Score Comparison:\")\n",
    "print(f\"  • Original data: {lr_original['F1-Score']:.4f}\")\n",
    "print(f\"  • Class weights: {lr_balanced['F1-Score']:.4f}\")\n",
    "print(f\"  • SMOTE: {lr_smote['F1-Score']:.4f}\")\n",
    "\n",
    "if lr_smote['F1-Score'] < lr_original['F1-Score']:\n",
    "    print(f\"  ⚠️ SMOTE hurt Logistic Regression performance!\")\n",
    "    print(f\"     F1-Score dropped by {lr_original['F1-Score'] - lr_smote['F1-Score']:.3f}\")\n",
    "    print(f\"     This suggests original data distribution is important for this model.\")\n",
    "\n",
    "print(f\"\\n💡 RECOMMENDATIONS:\")\n",
    "\n",
    "# Determine best approach for each model\n",
    "best_lr_approach = max([lr_original, lr_balanced, lr_smote], key=lambda x: x['F1-Score'])\n",
    "best_rf_approach = max([r for r in all_results if 'Random Forest' in r['Model']], key=lambda x: x['F1-Score'])\n",
    "\n",
    "print(f\"\\n  For Logistic Regression:\")\n",
    "if 'Original' in best_lr_approach['Model']:\n",
    "    print(f\"    • Use original data without rebalancing\")\n",
    "    print(f\"    • Consider threshold optimization instead\")\n",
    "elif 'Balanced' in best_lr_approach['Model']:\n",
    "    print(f\"    • Use class_weight='balanced' parameter\")\n",
    "    print(f\"    • This handles imbalance without synthetic data\")\n",
    "else:\n",
    "    print(f\"    • SMOTE works best for this model\")\n",
    "\n",
    "print(f\"\\n  For Random Forest:\")\n",
    "if 'Original' in best_rf_approach['Model']:\n",
    "    print(f\"    • Use original data - Random Forest handles imbalance well\")\n",
    "elif 'Balanced' in best_rf_approach['Model']:\n",
    "    print(f\"    • Use class_weight='balanced' parameter\")\n",
    "else:\n",
    "    print(f\"    • SMOTE provides the best results\")\n",
    "\n",
    "print(f\"\\n  General Recommendations:\")\n",
    "print(f\"    • Deploy: {best_result['Model']}\")\n",
    "print(f\"    • Focus on top {min(10, len(selected_features))} most important features\")\n",
    "if 'best_threshold' in locals() and abs(best_threshold - 0.5) > 0.05:\n",
    "    print(f\"    • Use optimized threshold: {best_threshold:.3f} instead of default 0.5\")\n",
    "print(f\"    • Implement real-time monitoring for new fraud patterns\")\n",
    "print(f\"    • Regular model retraining recommended\")\n",
    "print(f\"    • Consider ensemble methods for further improvement\")\n",
    "\n",
    "print(f\"\\n📈 BUSINESS IMPACT:\")\n",
    "fraud_amount_avg = df[df['Class'] == 1]['Amount'].mean()\n",
    "potential_savings = best_result['TP'] * fraud_amount_avg\n",
    "missed_losses = best_result['FN'] * fraud_amount_avg\n",
    "print(f\"  • Average fraud amount: ${fraud_amount_avg:.2f}\")\n",
    "print(f\"  • Potential fraud prevented: ${potential_savings:,.2f}\")\n",
    "print(f\"  • Estimated missed losses: ${missed_losses:,.2f}\")\n",
    "print(f\"  • Model effectiveness: {potential_savings/(potential_savings+missed_losses)*100:.1f}%\")\n",
    "\n",
    "if best_result['FN'] > 0:\n",
    "    print(f\"\\n⚠️ IMPORTANT: {best_result['FN']} fraud cases were missed\")\n",
    "    print(f\"    Consider adjusting threshold to increase recall if business cost of missing fraud is high\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Why SMOTE May Hurt Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis of why SMOTE might hurt performance\n",
    "print(\"ANALYSIS: Why SMOTE May Reduce Performance in Fraud Detection\")\n",
    "print(\"=\" * 65)\n",
    "\n",
    "print(\"\\n🔬 POSSIBLE EXPLANATIONS:\")\n",
    "print(\"\\n1. REAL-WORLD DISTRIBUTION MATTERS:\")\n",
    "print(\"   • Credit card fraud is naturally rare (~0.17% of transactions)\")\n",
    "print(\"   • The model needs to learn this real-world distribution\")\n",
    "print(\"   • Artificial balancing can distort decision boundaries\")\n",
    "\n",
    "print(\"\\n2. SYNTHETIC DATA QUALITY:\")\n",
    "print(\"   • SMOTE creates synthetic minority class samples\")\n",
    "print(\"   • These may not capture real fraud patterns accurately\")\n",
    "print(\"   • Could introduce noise that confuses the classifier\")\n",
    "\n",
    "print(\"\\n3. MODEL-SPECIFIC EFFECTS:\")\n",
    "print(\"   • Logistic Regression is particularly sensitive to class distribution\")\n",
    "print(\"   • It learns probability estimates based on training data proportions\")\n",
    "print(\"   • Artificial balancing can lead to miscalibrated probabilities\")\n",
    "\n",
    "print(\"\\n4. EVALUATION METRICS:\")\n",
    "print(\"   • F1-score balances precision and recall\")\n",
    "print(\"   • If SMOTE increases false positives significantly, F1 drops\")\n",
    "print(\"   • Original imbalanced data may naturally optimize for precision\")\n",
    "\n",
    "print(\"\\n💡 BETTER ALTERNATIVES TO SMOTE:\")\n",
    "print(\"   • Use class_weight='balanced' in sklearn models\")\n",
    "print(\"   • Optimize decision threshold instead of rebalancing data\")\n",
    "print(\"   • Use cost-sensitive learning approaches\")\n",
    "print(\"   • Focus on improving feature engineering\")\n",
    "print(\"   • Consider ensemble methods that handle imbalance naturally\")\n",
    "\n",
    "# Show the performance drop quantitatively\n",
    "lr_performance_drop = lr_original['F1-Score'] - lr_smote['F1-Score']\n",
    "print(f\"\\n📉 PERFORMANCE IMPACT:\")\n",
    "print(f\"   • Logistic Regression F1-Score drop with SMOTE: {lr_performance_drop:.3f}\")\n",
    "print(f\"   • That's a {lr_performance_drop/lr_original['F1-Score']*100:.1f}% relative decrease!\")\n",
    "\n",
    "if lr_balanced['F1-Score'] > lr_smote['F1-Score']:\n",
    "    improvement = lr_balanced['F1-Score'] - lr_smote['F1-Score']\n",
    "    print(f\"   • Class weights perform {improvement:.3f} points better than SMOTE\")\n",
    "    print(f\"   • Recommendation: Use class_weight='balanced' instead of SMOTE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save comprehensive results\n",
    "results_summary = display_df_sorted.copy()\n",
    "results_summary.to_csv('fraud_detection_comprehensive_results.csv', index=False)\n",
    "print(\"Results saved to 'fraud_detection_comprehensive_results.csv'\")\n",
    "\n",
    "# Save feature selection results\n",
    "feature_df.to_csv('selected_features.csv', index=False)\n",
    "print(\"Selected features saved to 'selected_features.csv'\")\n",
    "\n",
    "# Save feature importance if available\n",
    "if 'feature_importance' in locals():\n",
    "    feature_importance.to_csv('feature_importance.csv', index=False)\n",
    "    print(\"Feature importance saved to 'feature_importance.csv'\")\n",
    "\n",
    "# Save threshold optimization results\n",
    "if 'threshold_df' in locals():\n",
    "    threshold_df.to_csv('threshold_optimization.csv', index=False)\n",
    "    print(\"Threshold optimization results saved to 'threshold_optimization.csv'\")\n",
    "\n",
    "print(\"\\n✅ Comprehensive analysis completed successfully!\")\n",
    "print(\"\\n📋 KEY TAKEAWAYS:\")\n",
    "print(f\"  • Best approach: {best_result['Model']}\")\n",
    "print(f\"  • Best F1-Score: {best_result['F1-Score']:.4f}\")\n",
    "if 'best_f1_optimized' in locals() and best_f1_optimized > best_result['F1-Score']:\n",
    "    print(f\"  • With threshold optimization: {best_f1_optimized:.4f}\")\n",
    "print(f\"  • {best_result['TP']} out of {best_result['TP']+best_result['FN']} fraud cases detected\")\n",
    "print(f\"  • {best_result['FP']} false alarms out of {best_result['TN']+best_result['FP']} normal transactions\")\n",
    "print(f\"  • SMOTE may not always improve performance - test alternatives!\")\n",
    "print(f\"  • Model ready for deployment with proper threshold and monitoring\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}